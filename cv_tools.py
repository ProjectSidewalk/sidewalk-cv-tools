#Created mainly by Devesh Sarda with code borrowed from Galen Weld and Kavi Devi, August 2019

#Import the neccessary modules 
import sys
import utils
import errno
from PIL import Image, ImageDraw, ImageFont, ImageColor
import numpy as np
import threading
from collections import defaultdict
from queue import Queue
import os
import csv 
try:
	from xml.etree import cElementTree as ET
except ImportError as e:
	from xml.etree import ElementTree as ET
import torchvision
from torchvision import datasets, models, transforms
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from TwoFileFolder import TwoFileFolder
from resnet_extended1 import extended_resnet18 
import pandas as pd
import numpy as np
import math


#The 5 different label types
pytorch_label_from_int = ["NoCurbRamp", "Null", "Obstacle", "CurbRamp", "SurfaceProblem"]


''' Uses the TwoFileLoader to load the data into the model and saves the results in a dictionary
	Inputs: dir_containing_crops the path to the directory containing the crops that need to be analyzed 
			model_path path  to the folder that contains both the .pt file and the .log file for the current model
	Returns: An dictionary of predictions indexed by both the pano_id of the label and the coordinates of the label
	and where each index has a list of 5 float values (representing the probability of the crop being each class) as
	the value. 
'''

def predict_from_crops(dir_containing_crops, model_path,verbose=False):
	#Transformations to apply to the input image before passing it to the model
	data_transform = transforms.Compose([
		transforms.Resize(256),
		transforms.CenterCrop(224),
		transforms.ToTensor(),
		transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
		])
	
	if verbose:
		print("Building dataset and loading model...")
	
	device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #Connect to GPU and if not then CPU

	#Load the images and thier associated JSON files 
	image_dataset = TwoFileFolder(dir_containing_crops, meta_to_tensor_version=2, transform=data_transform)
	dataloader    = torch.utils.data.DataLoader(image_dataset, batch_size=4, shuffle=True, num_workers=4)

	len_ex_feats = image_dataset.len_ex_feats
	dataset_size = len(image_dataset)

	panos = image_dataset.classes

	if verbose:
		print("Using dataloader that supplies {} extra features.".format(len_ex_feats))
		print("")
		print("Finished loading data. Got crops from {} panos.".format(len(panos)))
	
	#Load the model
	model_ft = extended_resnet18(len_ex_feats=len_ex_feats)
	try:
		model_ft.load_state_dict(torch.load(model_path))
	except RuntimeError as e:
		model_ft.load_state_dict(torch.load(model_path, map_location='cpu'))
	model_ft = model_ft.to( device )
	optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

	model_ft.eval()

	paths_out = []
	pred_out  = []

	if verbose:
		print("Computing predictions....") 
	
	#Get the output predictions 
	for inputs, labels, paths in dataloader:
		inputs = inputs.to(device)
		labels = labels.to(device)
		
		optimizer_ft.zero_grad()
		with torch.set_grad_enabled(False):
			outputs = model_ft(inputs)
			_, preds = torch.max(outputs, 1)

			paths_out += list(paths)
			pred_out  += list(outputs.tolist())

	#Write predictions to the dictionary 
	predictions = defaultdict(dict)
	for i in range(len(paths_out)):
		path  = paths_out[i]
		preds = pred_out[i]

		_, filename = os.path.split(path)
		filebase, _ = os.path.splitext(filename)
		pano_id, coords = filebase.split('crop')

		predictions[pano_id][coords] = preds

	return predictions

'''
	Uses the prediction of dictionaries from the computer vision systems and writes them to the complete labels file 
	Inputs: predictions_dict The dictionary of computer vision predictions generated by the predict_from_crops method 
			root_path The path to the folder in which the file containing raw information needs to be saved 
			pred_file_name The name of the file which is going to save all the information 
			save_id if set to True then each row will contain the pano_id of the label 
	Returns: The path to the file that contains the predictions 
	Writes a .csv file where each row has the format pano_id, sv_x, sv_y, prediction (which is raw confidence values) for label]
'''
def write_predictions_to_file(predictions_dict, root_path, pred_file_name, verbose=False, save_id = True):
	path = os.path.join(root_path,pred_file_name)
	if os.path.exists(path):
		os.remove(path)
	with open(path, 'w', newline='') as csvfile:
			writer = csv.writer(csvfile)
			for pano_id in predictions_dict.keys():
				predictions = predictions_dict[pano_id]
				count = 0
				for coords, prediction in predictions.items():
					#Converts prediction to list
					if type(prediction) != list:
						prediction = list(prediction)
					x,y = coords.split(',')
					if(save_id == True):
						row = [pano_id] + [x,y] + prediction
					else:
						row = [x,y] + prediction
					writer.writerow(row)
					count += 1
			if verbose:
				print("\tWrote {} predictions to {}.".format(count + 1, path))
	return path

'''
	Runs the computer vision system on every crop in the single/crops folder 
	and writes the output to the single/completelabels file to be used later in the program
	Inputs: crop_dir The path to the directory that contains all the crops that need to be analyzed 
			path_dir The path to the directory that is going to store the completelabels file 
			model_dir The path to the directory that stores the .pt file and the .log file of the model
	Returns: Nothing
	Throws a FileNotFoundError if can't find any model in the model_dir
'''

def single_crops(crop_dir,path_dir,model_dir, verbose=False):
	model_name = utils.get_model_name()
	if model_name == None: 
		raise  FileNotFoundError(errno.ENOENT, "Could not find a model in directory", model_dir)
	model_path = os.path.join(model_dir, model_name+'.pt')
	preds = predict_from_crops(crop_dir,model_path,verbose=verbose)
	preds_loc = write_predictions_to_file(preds,path_dir,"completelabels.csv", verbose=verbose)

'''
	Uses the meta data file to extract the width, height, and pano_yaw_degree of a crop
	Inputs: path_to_metadata_xml The path to the meta data of the pano for which the info is needed
	Returns: a tuple containing the follow information: (width of image, height of image, pano yaw deg)
'''

def get_data(path_to_metadata_xml):
	pano = {}
	pano_xml = open(path_to_metadata_xml, 'rb')
	tree = ET.parse(pano_xml)
	root = tree.getroot()
	for child in root:
		if(child.tag == 'data_properties' or child.tag == 'projection_properties'):
			pano[child.tag] = child.attrib
	return [pano['data_properties']['image_width'], pano['data_properties']['image_height'], pano['projection_properties']['pano_yaw_deg']]


'''
Given a pano_id and a list of streetview x and y coordinatees of user labels, creates crops around those points 
using the utils.make_single_crop() method and saves those images in the single/crops folder. 
Input: predictions 	A list of strings each of which is a coordinate pair representing the location of the label 
example: ["100,-200", "-300, 400", ...]
	   pano The pano_id of the image on which the label was placed 
	   path_to_panos The complete path to the directory which contains the pano in the format specified at 
	   https://github.com/ProjectSidewalk/sidewalk-cv-tools
Returns: A 1 if the crops was sucessfully created and 0 if the crop could not be made

'''

def make_crop(predictions, pano, path_to_panos): 
	complete_path = os.path.join('single','crops')
	if not os.path.exists(complete_path):
		os.makedirs(complete_path)
	#Get path to pano and meta data
	path_to_pano = os.path.join(path_to_panos,pano[:2],pano)
	image =  path_to_pano + ".jpg"
	im = None
	#Extract the image
	if os.path.exists(image):
		try:
			im = Image.open(image)
		except Exception as ex:
			template = "An exception of type " + str(type(ex).__name__) + " occured importing image: " + str(image)
			print(template)
	else: 
		return 0
	xml = path_to_pano + ".xml"
	depth_name = path_to_pano + ".depth.txt"
	depth = None
	#Get the depth data 
	with open(depth_name, 'rb') as f:
		depth = np.loadtxt(f)
	#Get width, height, and yawdeg from metadata of pano
	data = get_data(xml)
	width = int(data[0])
	height = int(data[1])
	yawdeg = float(data[2])
	m = 0
	if not os.path.exists(complete_path):
		os.makedirs(complete_path)
	#Make the crop around the given points
	for prediction in predictions:
		prediction = prediction.strip()
		output = os.path.join(complete_path,pano + "_crop" + str(prediction))
		coord = prediction.split(',')
		imagex = float(coord[0])
		imagey = float(coord[1])
		if im != None:
			utils.make_single_crop(im,width,height,depth,pano, imagex, imagey, yawdeg, output)
			m += 1
	return m

crops_total = 0

'''
Calls the make_crop method for the pano_id extracted from the queue and displays information about progress
to the user. Mainly used to speed up the crop creating process 
Inputs: queue A queue where each item is a list which has the following format [pano_id, coordinate_pair1, coordinate_pair2, ...]
example: ["1a1UlhadSS_3dNtc5oI10Q","100,-200", "-300, 400", ...]
		path_to_panos The complete path to the directory which contains the pano in the format specified at  https://github.com/ProjectSidewalk/sidewalk-cv-tools
Returns: Nothing
'''

def thread_crop_image(queue_of_panos, path_to_panos):
	global crops_total
	while not queue_of_panos.empty():
		row = queue_of_panos.get()
		pano_id = row.pop(0)
		crops_total += make_crop(row, pano_id, path_to_panos)
		#Updates the user about the progress of the crop making method 
		t = "\rCrops made so far is " + str(crops_total)
		sys.stdout.write(t) 
		sys.stdout.flush()

'''
Puts each pano along with the coordinates of the user labels for that pano in to a queue and 
runs multiple threads to make the necessary crops
Inputs: dict_image A dictionary where the key is the pano_id ("1a1UlhadSS_3dNtc5oI10Q") and the value is
a list of strings representing the coordinates ["100,-200", "-300, 400", ...]
		path_to_panos The complete path to the directory which contains the panos in the format specified at  https://github.com/ProjectSidewalk/sidewalk-cv-tools
		num_threads The number of threads to make and it depends on the hardware specificiations of the device.  The num_threads value is equal to # of logical processors
		of your device which can be found at Task Manager -> Peformance -> CPU. 
'''

def make_crop_threading(dict_image, path_to_panos, verbose, num_threads): 
	q = Queue()
	count = 0
	#Puts items into the queue
	for pano_id, coords in dict_image.items():
		complete = [pano_id] + coords
		q.put(complete)
		count += 1
	threads = []
	#Starts num_threads number of threads  
	for i in range(num_threads):
		t = threading.Thread(target = thread_crop_image, args=(q, path_to_panos, ))
		threads.append(t)
		t.start()
	#End the program only after all the threads finish
	for proc in threads:
		proc.join()

path_to_completelabels = os.path.join("single", "completelabels.csv")

'''
Calls the single_crops() method to generate complete labels file from the crops made by the make_crop_threadings() method
Inputs: verbose If True prints information about the process to console 
Returns: Nothing 
'''

def get_results(verbose):
	if(len(os.listdir(os.path.join("single", "crops"))) > 0):
		if os.path.exists(path_to_completelabels):
			os.remove(path_to_completelabels)
		if(verbose):
			print("Delted a old compeltelabels file")
		single_crops("single","single", "models", verbose=True)
	elif(verbose):
		print("No new crops to run CV")

'''
Extracts information about user label and cv predictions along with prediction values from the complete labels file 
Inputs: ignore_null: If true then the null value is not counted when findiing the highest confidence label
Returns: A 2D array in which each array has the following format: [pano_id, sv_x, sv_y, prediction_label, confidence_value]
where prediction_label is the label that has the highest confidence and the confidence_value is the confidence of the 
prediction_label. 
'''

def read_complete_file(ignore_null):
	rows = []
	if os.path.exists(path_to_completelabels):
		with open(path_to_completelabels, 'r') as csvfile: 
			csvreader = csv.reader(csvfile) 
			for row in csvreader: 
				if(len(row) == 0):
					continue
				value = row[:3]
				numbers = list(map(float, row[3:]))
				removed = numbers.copy()
				if ignore_null: 
					removed.pop(1)
				confidence_value = max(removed) #Get max confidence number 
				prediction_label = pytorch_label_from_int[numbers.index(confidence_value)] #Get label type of max confidence number
				confidence_value = round(confidence_value, 2)
				value.append(prediction_label)
				value.append(confidence_value)
				rows.append(value)
	return rows

'''
Creates a dictionary of cv_predictions from the results of the read_complete_file method() 
Inputs: ignore_null: If true then the null value is not counted when findiing the highest confidence label
Returns: A dictionary where the key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,100,-200")
and the value is the highest prediction label + "," + label_percentage ("CurbRamp, 2.43") for each item in the
cv_labels file 
'''

def exact_labels(ignore_null):
	rows = read_complete_file(ignore_null) 
	dict = {}
	for row in rows:
		file_name = row[0][:- 1]
		x = row[1]
		y = row[2]
		label = row[3]
		percentage = float(row[4])
		key = file_name + "," + str(x) + "," + str(y) 
		result = label + "," + str(percentage)
		dict[key] = result
	return dict

'''
Method to create a pandas dataframe of the complete labels file generated in the write_predictions_to_file() method 
Inputs: Nothing 
Returns: A pandas data frame containing the complete labels file with column titles ["pano_id", "sv_x", "sv_y", "NoCurbRamp", "Null", "Obstacle", "CurbRamp", "SurfaceProblem"]
where the titles with label types representing the confidence value for that label type and the datafram is indexed by ['pano_id', 'sv_x', 'sv_y']
'''

def read_complete(): 
	df = pd.read_csv(path_to_completelabels, header = None)
	df.columns = ["pano_id", "sv_x", "sv_y", "NoCurbRamp", "Null", "Obstacle", "CurbRamp", "SurfaceProblem"]
	df.set_index(['pano_id', 'sv_x', 'sv_y'], inplace=True)
	return df 

'''
	Method to get higher scores for lower values of confidence while mapping (-infinity, infinity) to (0, 1)
	Input: x value corresponding to the confidence of the prediciton value
	Output: The score for that confidence value 
'''

def get_sigmoid(x): 
    return 1.0/(1.0 + math.exp(0.5 * (x - 2.5)))

'''
	Method to get lower values for low x values (difference between cv_confidence and user_confidence) while mapping (0, infiinity) to (0, 1)
	Input: value the difference between the cv_prediction and user_prediction
	Outputs: The score for the given difference value 
'''

def second_sigmoid(value): 
    return 1.0/(1.0 + math.exp(-0.6 * value + 3.0))

'''
	Calculates a priority score for the given label type for the importance of this label being validated by humans
	Inputs: cv_label The label type that the model believes is most likely to be present at the given location
			cv_confidence The confidence value returned by the CV system associated with the cv_label type
			user_label is the label type placed the user that placed the label 
			user_confidence is the confidence value returned by the CV system for the user_label
	Returns: A priority score betweeen (0,1) where 0 is low priority and 1 is high priority 
'''

def get_score(cv_label,cv_confidence, user_label, user_confidence): 
    if(cv_label == user_label): 
        return get_sigmoid(float(cv_confidence))
    return 0.15 * second_sigmoid(float(cv_confidence) - float(user_confidence)) + 0.35 * get_sigmoid(float(user_confidence)) + 0.5

user_data = {}

'''
Method to write the summary.csv file which contains information about cv's predictions on the labels included in the input file
Inputs: rows_dict A dictionary where the key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,100,-200")
and the value is the highest prediction label + "," + label_percentage ("CurbRamp, 2.43")
		labels_list A 2D array of labels present in the originial input file where each row has the format [pano_id, sv_x, sv_y]
example: ["1a1UlhadSS_3dNtc5oI10Q", 100,-200]
		add_to_summary A dictionary of labels that have previously been analyzed by the CV system where the key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,100,-200")
and the value is a list of format [CV_Label (label type with max confidence), confidence (confidence value for cv_label), raw values of confidences for the 5 label types]
example [Curb Ramps, 2.5, -2.3, 1.2, 2.5, 0.1, -1.6]
		path_to_summary The path to the directory where the summary.csv file is going to be saveds 
Returns: A 2D array containing information about labels that the system analyzed for the first time and thus doesn't need to analyzed any more where each row has the format [pano_id, sv_x, sv_y, 
CV_Label (label type with max confidence), confidence (confidence value for cv_label), raw values of confidences for the 5 label types]  example ["1a1UlhadSS_3dNtc5oI10Q", -300, 400, Curb Ramps, 
2.5, -2.3, 1.2, 2.5, 0.1, -1.6]
Writes a summary.csv file where each row has the format [label_id, label type that cv believes to present at the location, confidence value for cv_label,
label type assigned when the user placed the label, confidence value for user label type, priority score for label to be validated, raw confidence value for each class]
'''

def write_summary_file(rows_dict, labels_list , add_to_summary, path_to_summary):
	global user_data
	new_lables = [] 
	raw_values = None
	if os.path.exists(path_to_completelabels): 
		raw_values = read_complete()
	additional = []
	for item in pytorch_label_from_int: 
		add = item + "_confidence"
		additional.append(add)
	title = ["label_id", "cv_label", "cv_confidence", "user_label", "user_label_confidence", "priority_score"] + additional
	#Writting titles to the columns 
	name_of_summaryfile = os.path.join(path_to_summary,"summary.csv")
	if os.path.exists(name_of_summaryfile):
		os.remove(name_of_summaryfile)
	with open(name_of_summaryfile, 'w+', newline='') as csvfile:
			writer = csv.writer(csvfile)
			writer.writerow(title)
			#Adding items that have previously being processed by the CV systems and the resutls are saved in an already.csv file
			for first, value in add_to_summary.items():
				pano_id, x, y = first.split(",")
				x = int(float(x))
				y = int(float(y)) 
				values = value[2:]
				cvlabel = value[0]
				confidence = value[1] #Getting the confidence values of the CV system
				complete = pano_id + "," + str(float(x)) + "," + str(float(y))
				for label_id, userlabel in user_data[complete]:
					value = float(values[pytorch_label_from_int.index(userlabel)]) 
					score = get_score(cvlabel, confidence, userlabel, value) #Getting priority score
					row = [label_id, cvlabel, confidence, userlabel, value, score] + values
					writer.writerow(row)
			#Writing summary information for labels that have not been previously processed by the CV system 
			for labelrow in labels_list:
				pano_id = labelrow[0]
				x = float(labelrow[1])
				y = float(labelrow[2])
				complete = pano_id + "," + str(x) + "," + str(y)
				if(complete in rows_dict and complete not in add_to_summary):
					first_label = labelrow[3]
					index = (pano_id + "_", x, y)
					values = raw_values.loc[index, :]
					cv_respone = rows_dict[complete]
					label,confidence  = cv_respone.split(",") #Getting information about CV response 
					for label_id, user_label in user_data[complete]:
						user_confidence = round(float(values[user_label]), 2)
						score = get_score(label, confidence, user_label, user_confidence) #Getting priority score
						raw_stuff = list(values)
						rounded_stuff = [round(float(item), 2) for item in raw_stuff] 
						display = [label_id, label, confidence]  + [user_label, user_confidence, score] + rounded_stuff
						writer.writerow(display)
						#Write information about prediction to new_labels list
						value = [pano_id, int(x), int(y), label, confidence] + rounded_stuff		
						new_lables.append(value)			
	return new_lables

'''
Filters out the labels for which predictions have previously being made (found in an already.csv file)
 and batches labels for the same pano_id together
Inputs: dict_valid A dictionary where the key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,-300,400") and the value 
is the consensus label_type at that location (Surface Problem)
		existing_labels A dictionary of labels that have previously been analyzed by the CV system where the key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,100,-200")
and the value is a list of format [CV_Label (label type with max confidence), confidence (confidence value for cv_label), raw values of confidences for the 5 label types]
Returns: A dictionary where the key is pano_id ("1a1UlhadSS_3dNtc5oI10Q") and the value is a list of strings for each location to bee analyzed ["100,-200", "-300, 400"]
'''

def generate_image_date(dict_valid, existing_labels, verbose): 
	dict_image = {}
	for key in dict_valid: 
		pano_id, x, y = key.split(",")
		complete = pano_id + "," + str(float(x)) + "," + str(float(y))
		if complete in existing_labels:
			#Already analyzed that location
			continue
		test = x + "," + y
		if pano_id in dict_image:
			dict_image[pano_id].append(test)
		else:
			dict_image[pano_id] = [test]
	total = 0
	for pano_id, points in dict_image.items():
		total += len(points)
	if(verbose):
		print("Number of new labels to consider: " + str(total))
	return dict_image

'''
Converts a dictionary of user labels into a list of user labels 
Inputs: A dictionary with the key pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,-300,400")
and the value is the consensus label type for that location
Returns: A 2D array where each row has the format [pano_id, sv_x, sv_y, consenus label type] example:
["1a1UlhadSS_3dNtc5oI10Q", 100, -200, Curb Ramp]
'''

def generate_labelrows(dict_row):
	labelrow = []
	for complete_name,label in dict_row.items():
		pano_id, x, y = complete_name.split(",")
		label = [pano_id, x, y, label]
		labelrow.append(label)
	return labelrow

'''
Method to read data from the input file containing user labels. Breakes them up into labels
that have already been validated by the CV system and labels that need to be ran past the CV 
system. Converts the input data into a format that is used throghout the program and also batches
all the labels at a particular location together so that we don't run the CV system on the same
crop twice
Inputs: path The path to the file that contains user_label in the following format [time_stamp,
label_id, pano_id, sv_x, sv_y, user_label]
		date_after Makes the program only look at labels with time_stamps after the date_after value
but deafulted to include all the labels
		existing_labels A dictionary of labels that have already been analyzed by the cv system with 
 key pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,-300,400") and the value is a list of format 
 [CV_Label (label type with max confidence), confidence (confidence value for cv_label), raw values of confidences for the 5 label types]
		add_to_summary A dictionary to which every label that has already been analyzed and needs to included in
the summary file and has key and value the same as the existing labels dicitonary 
		number_agree The minimum number of people (including the label placer) that need to agree with the consensus to consider the 
location but the default is set to consider all the labels 
Returns: A dictionary with the key  pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,-300,400") and the value being the 
consensus label type for that locations
Throws a FileNotFoundError if the path to labels file does not exist
'''

def read_validation_data(path, date_after, existing_labels, add_to_summary, number_agree, verbose):
	global user_data
	dict = {}
	updated ={}
	totalcount = 0
	if os.path.exists(path):
		with open(path) as csvfile: 
			csvreader = csv.reader(csvfile, delimiter=',') 
			next(csvreader)
			for row in csvreader:
				time_stamp = row[0]
				date, time = time_stamp.split(" ")
				#Only proceed if the label occurs after the date_after value
				if(date > date_after):
					totalcount += 1
					#Extarct info about user label
					label_id = row[1]
					pano_id = row[2]
					x = row[3]
					y = row[4]
					complete = pano_id + "," + str(float(x)) + "," + str(float(y))
					label = row[5]
					if not(complete in dict):
						#Add location if it has has other user labels
						dict[complete] = [complete in existing_labels, label] 
						user_data[complete] =[(label_id, label)]
					else:
						#Add label if it is the first label in that location 
						dict[complete].append(label)
						user_data[complete].append((label_id, label))
			counter = 0
			for key,predictions in dict.items():
				count = [0, 0, 0, 0, 0]
				counter += 1
				add = predictions.pop(0)
				#Get crowd consensus on the label type of the location and set that as a temporary label
				for pred in predictions:
					count[pytorch_label_from_int.index(pred)] += 1
				maxval = np.argmax(count)
				if(count[maxval] < number_agree):
					continue
				labeltype = pytorch_label_from_int[maxval]
				if(add): 
					#Location already processed by CV system
					row = existing_labels[key]
					add_to_summary[key] = row
				else:
					#Location that needs to be processed by CV system
					updated[key] =  labeltype
	else:
		raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)
	if(verbose):
		print("Already have results for " + str(len(add_to_summary)) + " items")
		print("Need to get results for: " + str(len(updated)))
	return updated

'''
	Extracts information about all the labels that the CV systems has previously processed 
	so that we don't need to process to the locations again
	Input: path_to_panos The complete path to the directory which contains the panos in the format specified at  https://github.com/ProjectSidewalk/sidewalk-cv-tools
	Returns: A dictionary whose key is pano_id + "," + sv_x + "," + sv_y ("1a1UlhadSS_3dNtc5oI10Q,-300,400") and the value is a list of format [CV_Label 
(label type with max confidence), confidence (confidence value for cv_label), raw values of confidences for the 5 label types]
'''

def labels_already_made(path_to_panos):
	dict = {}
	sub = [x[0] for x in os.walk(path_to_panos)]
	sub.pop(0)
	for file in sub: 
		name_of_existing_file = os.path.join(file,"already.csv")
		if(os.path.exists(name_of_existing_file)):
			with open(name_of_existing_file) as csvfile:
				csvreader = csv.reader(csvfile, delimiter=',') 
				next(csvreader)
				for row in csvreader:
					pano_id = row[0]
					x = row[1]
					y = row[2]
					potential = pano_id + "," + str(float(x)) + "," + str(float(y))
					values = row[3:]
					dict[potential] = values
	return dict

'''
	Stores information about the labels that have been processed by the CV system in this iteration so that 
	we can load the information next time in already.csv files for each pano group
	Inputs: new_labels A 2D array of all the first time labels with each row having format [pano_id, sv_x, sv_y, 
CV_Label (label type with max confidence), confidence (confidence value for cv_label), raw values of confidences 
for the 5 label types]  example ["1a1UlhadSS_3dNtc5oI10Q", -300, 400, Curb Ramps, 2.5, -2.3, 1.2, 2.5, 0.1, -1.6]
'''

def update_labels_already_made(new_lables,path_to_panos):
	writer = None
	for row in new_lables:
		pano_id = row[0]
		complete = os.path.join(path_to_panos,pano_id[:2],"already.csv")
		exist = os.path.exists(complete)
		with open(complete, 'a+', newline='') as csvfile:
			writer = csv.writer(csvfile)
			if not exist: 
				#Add column titles to the file if it was just know created
				title = ["pano_id", "sv_x", "sv_y", "cv label", "cv confidence"] + pytorch_label_from_int
				writer.writerow(title)
			if(writer != None):
				writer.writerow(row)

'''
Main method that calls all the above methods to generate the summary.csv file from the input labels file and all the
steps in between. Please see  the validation section of the README at https://github.com/ProjectSidewalk/sidewalk-cv-tools 
for information about each of the inputs
Returns Nothing
'''

def generate_data(input_data, date_after,path_to_panos, ignore_null, number_agree, path_to_summary, verbose, num_threads):
	crops = os.path.join(path_to_summary,"crops")
	if not os.path.exists(crops):
		os.makedirs(crops)
	utils.clear_dir(crops)
	existing_labels = labels_already_made(path_to_panos) #Grab the locations already processed by CV
	add_to_summary = {}
	dict_valid = read_validation_data(input_data, date_after, existing_labels, add_to_summary, number_agree, verbose) #List of labels that need to processed
	dict_image = generate_image_date(dict_valid, existing_labels, verbose) #Format the data 
	make_crop_threading(dict_image, path_to_panos, verbose, num_threads) #Make the necessary crops 
	#Process the CV predictions 
	get_results(verbose)
	rows_dict = exact_labels(ignore_null)
	labels_list = generate_labelrows(dict_valid)
	new_labels = write_summary_file(rows_dict, labels_list, add_to_summary, path_to_summary) #Write the summary file 
	if(verbose):
		print("Number of new labels is " + str(len(new_labels)))
	#update_labels_already_made(new_labels,path_to_panos) #Save the locations that have been currently processed 
	utils.clear_dir(crops)
	if os.path.exists(path_to_completelabels):
		os.remove(path_to_completelabels)

'''
Wrapper class that interacts with the users whose main function is to the call the generate_data method
and has all the optional values loaded. 
Returns the path to the summary file and please write_summary_file or https://github.com/ProjectSidewalk/sidewalk-cv-tools 
about more information about the output.
Throws a FileNotFoundError if the path_to_panos or the input_data file does not exists
'''

def generate_validation_data(input_data,path_to_panos,path_to_summary, number_agree = 1,num_threads = 4, date_after = "2008-06-28", verbose = False):
	if not os.path.isdir(path_to_panos):
		raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path_to_panos)
	if not os.path.exists(input_data):
		raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), input_data)
	ignore_null = True 
	generate_data(input_data, date_after, path_to_panos, ignore_null, number_agree, path_to_summary, verbose, num_threads)
	return (os.path.join(path_to_summary,"summary.csv"))
